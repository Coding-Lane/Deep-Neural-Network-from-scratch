{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L layered Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "![img](images/deep_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(1, 209)\n",
      "(12288, 50)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt('dataset/cat_train_x.csv', delimiter = ',')/255.0\n",
    "Y_train = np.loadtxt('dataset/cat_train_y.csv', delimiter = ',').reshape(1, X_train.shape[1])\n",
    "X_test = np.loadtxt('dataset/cat_test_x.csv', delimiter = ',')/255.0\n",
    "Y_test = np.loadtxt('dataset/cat_test_y.csv', delimiter = ',').reshape(1, X_test.shape[1])\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randrange(0, X_train.shape[1])\n",
    "plt.imshow(X_train[:, index].reshape(64,64, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def softmax(z):\n",
    "    expZ = np.exp(z)\n",
    "    return expZ/(np.sum(expZ, 0))\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def derivative_relu(Z):\n",
    "    return np.array(Z > 0, dtype = 'float')\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "\n",
    "- We need to initialize the **W** parameters randomly, and **B** with zeros\n",
    "- And as our Deep Neural network has **L layers**, we will repeat it for **L-1 times**, from $W_1 to W_L$\n",
    "\n",
    "<img src=\"images/params.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) #*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W1: (100, 12288)\n",
      "Shape of B1: (100, 1) \n",
      "\n",
      "Shape of W2: (200, 100)\n",
      "Shape of B2: (200, 1) \n",
      "\n",
      "Shape of W3: (1, 200)\n",
      "Shape of B3: (1, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [X_train.shape[0], 100, 200, Y_train.shape[0]]\n",
    "params = initialize_parameters(layer_dims)\n",
    "\n",
    "for l in range(1, len(layer_dims)):\n",
    "    print(\"Shape of W\" + str(l) + \":\", params['W' + str(l)].shape)\n",
    "    print(\"Shape of B\" + str(l) + \":\", params['b' + str(l)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected :\n",
    "\n",
    "Shape of W1: (100, 12288)\n",
    "\n",
    "Shape of B1: (100, 1) \n",
    "\n",
    "Shape of W2: (200, 100)\n",
    "\n",
    "Shape of B2: (200, 1) \n",
    "\n",
    "Shape of W3: (1, 200)\n",
    "\n",
    "Shape of B3: (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "- sigmoid activation function will be used only at the last layer (output layer), while we will use relu/tanh for other layers\n",
    "\n",
    "<img src=\"images/forward_prop.png\" width=\"300\"/>\n",
    " \n",
    "\n",
    "For f(x), you can use either tanh or ReLU activation function. But also use the derivative of the same for Backpropagation as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activation):\n",
    "   \n",
    "    forward_cache = {}\n",
    "    L = len(parameters) // 2                  \n",
    "    \n",
    "    forward_cache['A0'] = X\n",
    "\n",
    "    for l in range(1, L):\n",
    "        forward_cache['Z' + str(l)] = parameters['W' + str(l)].dot(forward_cache['A' + str(l-1)]) + parameters['b' + str(l)]\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            forward_cache['A' + str(l)] = tanh(forward_cache['Z' + str(l)])\n",
    "        else:\n",
    "            forward_cache['A' + str(l)] = relu(forward_cache['Z' + str(l)])\n",
    "            \n",
    "\n",
    "    forward_cache['Z' + str(L)] = parameters['W' + str(L)].dot(forward_cache['A' + str(L-1)]) + parameters['b' + str(L)]\n",
    "    \n",
    "    if forward_cache['Z' + str(L)].shape[0] == 1:\n",
    "        forward_cache['A' + str(L)] = sigmoid(forward_cache['Z' + str(L)])\n",
    "    else :\n",
    "        forward_cache['A' + str(L)] = softmax(forward_cache['Z' + str(L)])\n",
    "    \n",
    "    return forward_cache['A' + str(L)], forward_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A0 : (12288, 209)\n",
      "Shape of A1 : (100, 209)\n",
      "Shape of A2 : (200, 209)\n",
      "Shape of A3 : (1, 209)\n"
     ]
    }
   ],
   "source": [
    "aL, forw_cache = forward_propagation(X_train, params, 'relu')\n",
    "\n",
    "for l in range(len(params)//2 + 1):\n",
    "    print(\"Shape of A\" + str(l) + \" :\", forw_cache['A' + str(l)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected :\n",
    "\n",
    "Shape of A0 : (12288, 209)\n",
    "\n",
    "Shape of A1 : (100, 209)\n",
    "\n",
    "Shape of A2 : (200, 209)\n",
    "\n",
    "Shape of A3 : (1, 209)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "#### For binary classification:\n",
    "$ Cost = - \\frac{1}{m} \\sum_{i=1}^{m} [ y*log(a_L) + (1-y)*log(1 - a_L) ] $\n",
    "\n",
    "#### For multi-class classification:\n",
    "\n",
    "$ Cost = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{i=k}^{n}[ y_k*log(a_k) ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    if Y.shape[0] == 1:\n",
    "        cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    else:\n",
    "        cost = -(1./m) * np.sum(Y * np.log(AL))\n",
    "        \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "\n",
    "- For last layer, **$dZ_L$** will be $A_L - Y$\n",
    "- Except for last layer, we use a loop to implement backprop for other layers\n",
    "\n",
    "<img src=\"images/backward_prop.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, parameters, forward_cache, activation):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(parameters)//2\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    grads[\"dZ\" + str(L)] = AL - Y\n",
    "    grads[\"dW\" + str(L)] = 1./m * np.dot(grads[\"dZ\" + str(L)],forward_cache['A' + str(L-1)].T)\n",
    "    grads[\"db\" + str(L)] = 1./m * np.sum(grads[\"dZ\" + str(L)], axis = 1, keepdims = True)\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        if activation == 'tanh':\n",
    "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_tanh(forward_cache['A' + str(l)])\n",
    "        else:\n",
    "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_relu(forward_cache['A' + str(l)])\n",
    "            \n",
    "        grads[\"dW\" + str(l)] = 1./m * np.dot(grads[\"dZ\" + str(l)],forward_cache['A' + str(l-1)].T)\n",
    "        grads[\"db\" + str(l)] = 1./m * np.sum(grads[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dZ3 : (1, 209)\n",
      "Shape of dW3 : (1, 200)\n",
      "Shape of dB3 : (1, 1) \n",
      "\n",
      "Shape of dZ2 : (200, 209)\n",
      "Shape of dW2 : (200, 100)\n",
      "Shape of dB2 : (200, 1) \n",
      "\n",
      "Shape of dZ1 : (100, 209)\n",
      "Shape of dW1 : (100, 12288)\n",
      "Shape of dB1 : (100, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(forw_cache[\"A\" + str(3)], Y_train, params, forw_cache, 'relu')\n",
    "\n",
    "for l in reversed(range(1, len(grads)//3 + 1)):\n",
    "    print(\"Shape of dZ\" + str(l) + \" :\", grads['dZ' + str(l)].shape)\n",
    "    print(\"Shape of dW\" + str(l) + \" :\", grads['dW' + str(l)].shape)\n",
    "    print(\"Shape of dB\" + str(l) + \" :\", grads['db' + str(l)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "\n",
    "Shape of dZ3 : (1, 209)\n",
    "\n",
    "Shape of dW3 : (1, 200)\n",
    "\n",
    "Shape of dB3 : (1, 1) \n",
    "\n",
    "Shape of dZ2 : (200, 209)\n",
    "\n",
    "Shape of dW2 : (200, 100)\n",
    "\n",
    "Shape of dB2 : (200, 1) \n",
    "\n",
    "Shape of dZ1 : (100, 209)\n",
    "\n",
    "Shape of dW1 : (100, 12288)\n",
    "\n",
    "Shape of dB1 : (100, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters\n",
    "<img src=\"images/update_params.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activation):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    y_pred, caches = forward_propagation(X, parameters, activation)\n",
    "    \n",
    "    if y.shape[0] == 1:\n",
    "        y_pred = np.array(y_pred > 0.5, dtype = 'float')\n",
    "    else:\n",
    "        y = np.argmax(y, 0)\n",
    "        y_pred = np.argmax(y_pred, 0)\n",
    "    \n",
    "    return np.round(np.sum((y_pred == y)/m), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Model\n",
    "\n",
    "Implement the entire Deep Neural Network here\n",
    "\n",
    "### Instructions :\n",
    "\n",
    "We need to initialize parameters once, and after that, we will run the following in a loop:\n",
    "- forward_prop(x, parameters)\n",
    "- cost_function(aL, y)\n",
    "- backward_prop(x, y, parameters, forward_cache)\n",
    "- parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "### Return :\n",
    "- parameters, which will be our trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.03, activation = 'relu', num_iterations = 3000):#lr was 0.009\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []              \n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, forward_cache = forward_propagation(X, parameters, activation)\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = backward_propagation(AL, Y, parameters, forward_cache, activation)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % (num_iterations/10) == 0:\n",
    "            print(\"\\niter:{} \\t cost: {} \\t train_acc:{} \\t test_acc:{}\".format(i, np.round(cost, 2), predict(X_train, Y_train, parameters, activation), predict(X_test, Y_test, parameters, activation)))\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"==\", end = '')\n",
    "\n",
    "       \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter:0 \t cost: 0.77 \t train_acc:0.51 \t test_acc:0.42\n",
      "==================================================\n",
      "iter:250 \t cost: 0.63 \t train_acc:0.74 \t test_acc:0.64\n",
      "==================================================\n",
      "iter:500 \t cost: 0.54 \t train_acc:0.78 \t test_acc:0.7\n",
      "==================================================\n",
      "iter:750 \t cost: 0.44 \t train_acc:0.92 \t test_acc:0.78\n",
      "==================================================\n",
      "iter:1000 \t cost: 0.32 \t train_acc:0.96 \t test_acc:0.8\n",
      "==================================================\n",
      "iter:1250 \t cost: 0.23 \t train_acc:0.98 \t test_acc:0.76\n",
      "==================================================\n",
      "iter:1500 \t cost: 0.16 \t train_acc:0.98 \t test_acc:0.82\n",
      "========================"
     ]
    }
   ],
   "source": [
    "layers_dims = [X_train.shape[0], 20, 7, 5, Y_train.shape[0]] #  4-layer model\n",
    "lr = 0.0075\n",
    "iters = 2500\n",
    "\n",
    "parameters = model(X_train, Y_train, layers_dims, learning_rate = lr, activation = 'relu', num_iterations = iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
